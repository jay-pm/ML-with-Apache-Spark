{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML with ApacheSpark_Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per Apache Spark official website spark has below advantages over other available tools (hadoop, mapreduce) in market:\n",
    "\n",
    "- **Speed**: Run workloads 100x faster [for both batch and streaming data]\n",
    "- **Ease of Use**: Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "- **Generality**: Combine SQL, streaming, and complex analytics.\n",
    "- **Runs Everywhere**: Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. It can access diverse data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Intro**\n",
    "\n",
    "Spark is a distributed computing platform for working with Big Data. It achieves efficiency by distributing data and computation across a cluster of computers.\n",
    "Spark is currently the best framework for cluster computing.\n",
    "Spark transparently handles the distribution of compute tasks across a cluster. Spark operations are fast and it also allows you to focus on the analysis rather than worry about technical details.\n",
    "\n",
    "Spark is popular because of below reasons: \n",
    "\n",
    "1.\tMuch faster than other big data technology like Hadoop because it does most processing in memory \n",
    "2.\tDeveloper friendly interface which hides much of the complexity of distributed computing [Spark has a high-level API, which conceals a lot of complexity.]\n",
    "3. Compute across a distributed cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture**\n",
    "\n",
    "- Each cluster contains one or more nodes [each node is a computer with cpu, ram and physical storage]\n",
    "- Cluster manager allocates resources and coordinates activities across the cluster\n",
    "- Every app running on spark cluster has a driver program \n",
    "- Using spark api driver communicates with cluster manager which in turn distribute works to the nodes\n",
    "- On each node spark launches an executor process which persists for the duration of the application\n",
    "- Work is divided into tasks which are simply unit of computation\n",
    "- Executor runs tasks in multiple threads across the cores in a node\n",
    "\n",
    "Sparks sets up all above infrastructure by itself and handle all interaction within the cluster. we don’t need to worry much about it. However it's still useful to know how does it work under the hood.\n",
    "\n",
    "\n",
    "<img src='sparkArchitecture.JPG' width="400" height="400" />\n",
    "\n",
    "image source: Datacamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting with Spark:**\n",
    "\n",
    "- The connection to spark is established by the driver which can be written either in Java, Scala, Python or R.\n",
    "- Java is relatively verbose which requires which requires a lot of code even for simple task.\n",
    "- Scala, Python and R are high level languages which can accomplish much with small amount of code. They also offer REPL [read, evaluate, print loop] which is crucial for interactive development.\n",
    "- python does not talk natively to spark hence we have to import pyspark which make spark functionality available in python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remote and Local cluster connection**:\n",
    "\n",
    "Before connecting with spark we need to tell spark where the cluster is located.\n",
    "\n",
    "    1.Remote cluster: We can connect to a remote cluster where we need to specify the URL for remote cluster. \n",
    "        URL format: spark://<IP address | DNS name>:<port>. \n",
    "        Default port is 7077 but we must specify this explicitly.\n",
    "    2.Local cluster [everythimng happens on a single computer]:\n",
    "        local — only 1 core;\n",
    "        local[2] — 2 cores; or\n",
    "        local[*] — all available cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation**\n",
    "- pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import and check version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Creating a spark session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the PySpark module\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a local cluster using a SparkSession builder**\n",
    "- specify location of cluster using \"master\" method\n",
    "- assign a name using \"appName\" method. This is optional.\n",
    "- \"getOrCreate\" method either will create a new session or return an existing session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Close session**\n",
    "\n",
    "When you are done with the session it is a good practice to close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkSession class has a **version** attribute which gives the version of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "# What version of Spark?\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session object will now allow us to load data into Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "- [Apache Spark](https://spark.apache.org/)\n",
    "- [towardsdatascience](https://towardsdatascience.com/deep-learning-with-apache-spark-part-1-6d397c16abd)\n",
    "- [Datacamp](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
